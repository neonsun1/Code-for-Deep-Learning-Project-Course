{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8360da76-432e-4c7e-a7cf-60b3e6f87ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92251b2d-d1ac-4555-b547-1cd4383b760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def downsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        def upsample(in_feat, out_feat, target_size=None, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if target_size is not None:\n",
    "                layers.append(nn.Upsample(size=target_size, mode='bilinear', align_corners=False))\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.ReLU())\n",
    "            return layers\n",
    "        \n",
    "        # 修改 self.model 的定义，在最后添加一层上采样以确保输出尺寸正确\n",
    "        self.model = nn.Sequential(\n",
    "            *downsample(channels, 64, normalize=False),\n",
    "            *downsample(64, 64),\n",
    "            *downsample(64, 128),\n",
    "            *downsample(128, 256),\n",
    "            *downsample(256, 512),\n",
    "            nn.Conv2d(512, 4000, 1),\n",
    "            *upsample(4000, 512),\n",
    "            *upsample(512, 256),\n",
    "            *upsample(256, 128),\n",
    "            *upsample(128, 64),\n",
    "            *upsample(64, channels, target_size=(128, 128)),  # 确保输出尺寸为 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x 是原始输入图像（包含完整的图像信息）\n",
    "        # mask 是掩码，指示哪些区域需要被修改\n",
    "        generated_content = self.model(x)\n",
    "\n",
    "        # 只更新掩码区域\n",
    "        output = x * (1 - mask) + generated_content * mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8efdef-1044-4d8c-95b7-e209a1bf86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff385688-dd15-4487-907b-32c30c687f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, category, transforms_=None, img_size=128, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        self.category = category\n",
    "        # 构建原始图像和掩码的路径\n",
    "        original_img_path = os.path.join(root, 'original_image', category)\n",
    "        mask_path = os.path.join(root, 'mask', category)\n",
    "\n",
    "        self.original_files = sorted(glob.glob(f\"{original_img_path}/*.jpg\"))\n",
    "        self.mask_files = sorted(glob.glob(f\"{mask_path}/*.jpg\"))  \n",
    "        self.transforms_mask = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),  # 确保掩码大小一致\n",
    "            transforms.ToTensor(),  # 转为张量\n",
    "        ])\n",
    "\n",
    "        assert len(self.original_files) == len(self.mask_files), \"The number of images and masks do not match!\"\n",
    "        \n",
    "\n",
    "    # def apply_random_mask(self, img):\n",
    "    #     \"\"\"Randomly masks image\"\"\"\n",
    "    #     y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n",
    "    #     y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
    "    #     masked_part = img[:, y1:y2, x1:x2]\n",
    "    #     masked_img = img.clone()\n",
    "    #     masked_img[:, y1:y2, x1:x2] = 1\n",
    "\n",
    "    #     return masked_img, masked_part\n",
    "\n",
    "    # def apply_center_mask(self, img):\n",
    "    #     \"\"\"Mask center part of image\"\"\"\n",
    "    #     # Get upper-left pixel coordinate\n",
    "    #     i = (self.img_size - self.mask_size) // 2\n",
    "    #     masked_img = img.clone()\n",
    "    #     masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
    "\n",
    "    #     return masked_img, i\n",
    "    def apply_mask(self, original_img, mask_img):\n",
    "        # 确保掩码是单通道且大小匹配原始图像\n",
    "        if mask_img.shape[0] == 1:  # 如果是单通道\n",
    "            mask_img = mask_img.expand(3, -1, -1)  # 扩展为 3 通道\n",
    "        \n",
    "        # 提取被遮挡部分的图像\n",
    "        masked_part = original_img * mask_img\n",
    "        \n",
    "        # 创建被遮挡的图像，用1填充遮挡区域\n",
    "        masked_img = original_img.clone()\n",
    "        masked_img[mask_img == 1] = 1  # 用1替换遮挡区域\n",
    "        \n",
    "        if mask_img.shape[0] == 1:  # 如果是单通道\n",
    "            mask_img = mask_img.expand(3, -1, -1)  # 扩展到 3 通道\n",
    "\n",
    "        return masked_img, masked_part\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 处理原始图像\n",
    "        img = Image.open(self.original_files[index]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # 处理掩码图像\n",
    "        mask_img = Image.open(self.mask_files[index]).convert(\"L\")  # 转为灰度图\n",
    "        mask_img = self.transforms_mask(mask_img)\n",
    "        \n",
    "        # 确保掩码是二值化的\n",
    "        mask_img = (mask_img > 0.5).float()  # 掩码值为 0 或 1\n",
    "    \n",
    "        return img, mask_img\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3738730-0fa7-4eac-8289-5d7683a11b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 200\n",
    "        self.batch_size = 8\n",
    "        self.dataset_name = \"img_align_celeba\"\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 4\n",
    "        self.latent_dim = 100\n",
    "        self.img_size = 128\n",
    "        self.mask_size = 64\n",
    "        self.channels = 3\n",
    "        self.sample_interval = 500\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Opt(n_epochs={self.n_epochs}, batch_size={self.batch_size}, \"\n",
    "                f\"dataset_name='{self.dataset_name}', lr={self.lr}, b1={self.b1}, \"\n",
    "                f\"b2={self.b2}, n_cpu={self.n_cpu}, latent_dim={self.latent_dim}, \"\n",
    "                f\"img_size={self.img_size}, mask_size={self.mask_size}, \"\n",
    "                f\"channels={self.channels}, sample_interval={self.sample_interval})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9158f68b-395a-41d9-b511-23d1c6ed64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1459534/2396011300.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(generator_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for face: {'PSNR': 10.96069994132162, 'SSIM': 0.3195035795960575, 'VIF': 0.8215660663227156}\n",
      "Test completed and images saved to ./output_images_gan/face\n",
      "Test completed and images saved to ./output_images_gan/face\n",
      "Metrics for scenario: {'PSNR': 10.267389511204204, 'SSIM': 0.2773144560400397, 'VIF': 0.7742339353464112}\n",
      "Test completed and images saved to ./output_images_gan/scenario\n",
      "Test completed and images saved to ./output_images_gan/scenario\n",
      "Metrics for street_scene_pairs: {'PSNR': 11.085944655448026, 'SSIM': 0.2836719263705891, 'VIF': 0.7692075080523576}\n",
      "Test completed and images saved to ./output_images_gan/street_scene_pairs\n",
      "Test completed and images saved to ./output_images_gan/street_scene_pairs\n",
      "Metrics for texture: {'PSNR': 9.46899983580659, 'SSIM': 0.26791315180948005, 'VIF': 0.840434243716509}\n",
      "Test completed and images saved to ./output_images_gan/texture\n",
      "Test completed and images saved to ./output_images_gan/texture\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "from sewar.full_ref import vifp  # 第三方库 sewar 提供 VIF 和 FSIM 的实现\n",
    "import numpy as np\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_metrics(original, generated):\n",
    "    \"\"\"\n",
    "    计算图像质量评估指标\n",
    "    :param original: 原始图像 (H, W, C) 格式\n",
    "    :param generated: 生成图像 (H, W, C) 格式\n",
    "    :return: 指标字典\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    \n",
    "    # # 确保图像被标准化到 [0, 1]\n",
    "    original = np.array(original).astype(np.float32) / 255.0\n",
    "    generated = np.array(generated).astype(np.float32) / 255.0    \n",
    "\n",
    "\n",
    "\n",
    "    # 如果图像只有单通道，增加通道维度\n",
    "    if original.ndim == 2:\n",
    "        original = np.expand_dims(original, axis=-1)\n",
    "    if generated.ndim == 2:\n",
    "        generated = np.expand_dims(generated, axis=-1)\n",
    "    \n",
    "    # PSNR\n",
    "    psnr_value = psnr(original, generated, data_range=1)\n",
    "\n",
    "     # SSIM \n",
    "    ssim_value, _ = ssim(\n",
    "    original, \n",
    "    generated, \n",
    "    channel_axis=-1,  # 指定通道轴为最后一维\n",
    "    data_range=1.0, \n",
    "    full=True, \n",
    "    win_size=7\n",
    ")\n",
    "\n",
    "    \n",
    "    # VIF\n",
    "    vif_value = vifp(original, generated)\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": psnr_value,\n",
    "        \"SSIM\": ssim_value,\n",
    "        \"VIF\": vif_value,\n",
    "    }\n",
    "    \n",
    "# 确保输出目录存在\n",
    "def adjust_image_size(image, target_size):\n",
    "    \"\"\"\n",
    "    调整图像大小以匹配目标尺寸。\n",
    "    :param image: 输入的PIL图像对象\n",
    "    :param target_size: 目标尺寸 (宽度, 高度)\n",
    "    :return: 调整大小后的PIL图像对象\n",
    "    \"\"\"\n",
    "    return image.resize(target_size, Image.ANTIALIAS)\n",
    "    \n",
    "opt = Opt()\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "def test_model(category, generator_path, output_dir = \"./output_images_gan\"):\n",
    "    # 加载训练好的生成器模型\n",
    "    output_dir += \"/\" + category\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    generator = Generator(channels=opt.channels)\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator.eval()\n",
    "    \n",
    "    if cuda:\n",
    "        generator.cuda()\n",
    "\n",
    "    # 准备测试数据集加载器\n",
    "    transforms_ = [\n",
    "        transforms.Resize((opt.img_size, opt.img_size), Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    "    test_dataloader = DataLoader(\n",
    "        ImageDataset(\"./dataset_test\", category, transforms_=transforms_, mode=\"val\"),\n",
    "        batch_size=12,\n",
    "        shuffle=False,  # 测试时无需打乱数据\n",
    "        num_workers=1,\n",
    "    )\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    metrics_sum = {\"PSNR\": 0.0, \"SSIM\": 0.0, \"VIF\": 0.0}\n",
    "    total_images = 0\n",
    "    \n",
    "\n",
    "    for i, (imgs, masks) in enumerate(test_dataloader):\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        masks = Variable(masks.type(Tensor))\n",
    "\n",
    "        # 创建掩码图像（生成器输入）\n",
    "        masked_imgs = imgs * (1 - masks) + masks\n",
    "\n",
    "        # 使用生成器预测修复后的图像\n",
    "        gen_imgs = generator(masked_imgs, masks)\n",
    "\n",
    "        # 保存生成的图像\n",
    "        for j in range(gen_imgs.shape[0]):\n",
    "            img_sample = gen_imgs[j].data\n",
    "            save_path = f\"{output_dir}/output_{category}_{i * len(gen_imgs) + j}.png\"\n",
    "            save_image(img_sample, save_path)\n",
    "\n",
    "            # 加载保存的生成图像和真实图像\n",
    "            generated_image = Image.open(save_path).convert(\"RGB\")\n",
    "            original_image = (imgs[j].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5) * 255\n",
    "            original_image = Image.fromarray(original_image.astype(np.uint8))\n",
    "\n",
    "            # 如果需要，调整输出图像大小以匹配原始图像\n",
    "            if generated_image.size != original_image.size:\n",
    "                original_image = adjust_image_size(original_image, generated_image.size)\n",
    "\n",
    "            # 计算指标\n",
    "            metrics = calculate_metrics(original_image, generated_image)\n",
    "            for key in metrics:\n",
    "                metrics_sum[key] += metrics[key]\n",
    "            total_images += 1\n",
    "            \n",
    "     # 计算平均指标\n",
    "    metrics_avg = {key: metrics_sum[key] / total_images for key in metrics_sum}\n",
    "\n",
    "    # 保存指标到文件\n",
    "    metrics_file = os.path.join(output_dir, f\"{category}_average_metrics_gan.json\")\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics_avg, f, indent=4)\n",
    "\n",
    "    print(f\"Metrics for {category}: {metrics_avg}\")\n",
    "    print(f\"Test completed and images saved to {output_dir}\")\n",
    "\n",
    "    print(f\"Test completed and images saved to {output_dir}\")\n",
    "\n",
    "# 假设我们只对 'face' 类型进行测试\n",
    "image_types = [\"face\", \"scenario\", \"street_scene_pairs\", \"texture\"]\n",
    "for image_type in image_types:\n",
    "    #train_model(image_type=image_type, num_epochs=10, batch_size=16, learning_rate=1e-5)\n",
    "    generator_path = f\"{image_type}_generator.pth\"\n",
    "    test_model(image_type, generator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ae9d9-e9ee-499d-b268-09a0869901a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "sd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
