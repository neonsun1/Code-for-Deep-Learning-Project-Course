{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa0c5b3-6508-4dbb-b0aa-f5b40454d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b06833-1a90-4e03-9eba-268b410905ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def downsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        def upsample(in_feat, out_feat, target_size=None, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if target_size is not None:\n",
    "                layers.append(nn.Upsample(size=target_size, mode='bilinear', align_corners=False))\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.ReLU())\n",
    "            return layers\n",
    "        \n",
    "        # 修改 self.model 的定义，在最后添加一层上采样以确保输出尺寸正确\n",
    "        self.model = nn.Sequential(\n",
    "            *downsample(channels, 64, normalize=False),\n",
    "            *downsample(64, 64),\n",
    "            *downsample(64, 128),\n",
    "            *downsample(128, 256),\n",
    "            *downsample(256, 512),\n",
    "            nn.Conv2d(512, 4000, 1),\n",
    "            *upsample(4000, 512),\n",
    "            *upsample(512, 256),\n",
    "            *upsample(256, 128),\n",
    "            *upsample(128, 64),\n",
    "            *upsample(64, channels, target_size=(128, 128)),  # 确保输出尺寸为 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x 是原始输入图像（包含完整的图像信息）\n",
    "        # mask 是掩码，指示哪些区域需要被修改\n",
    "        generated_content = self.model(x)\n",
    "\n",
    "        # 只更新掩码区域\n",
    "        output = x * (1 - mask) + generated_content * mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d531f4-3cdf-4621-b2e1-8f405395bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d003033-1122-40e7-a8f6-030417213edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, category, transforms_=None, img_size=128, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        self.category = category\n",
    "        # 构建原始图像和掩码的路径\n",
    "        original_img_path = os.path.join(root, 'original_image', category)\n",
    "        mask_path = os.path.join(root, 'mask', category)\n",
    "\n",
    "        self.original_files = sorted(glob.glob(f\"{original_img_path}/*.jpg\"))\n",
    "        self.mask_files = sorted(glob.glob(f\"{mask_path}/*.jpg\"))  \n",
    "        self.transforms_mask = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),  # 确保掩码大小一致\n",
    "            transforms.ToTensor(),  # 转为张量\n",
    "        ])\n",
    "\n",
    "        assert len(self.original_files) == len(self.mask_files), \"The number of images and masks do not match!\"\n",
    "        \n",
    "\n",
    "    # def apply_random_mask(self, img):\n",
    "    #     \"\"\"Randomly masks image\"\"\"\n",
    "    #     y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n",
    "    #     y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
    "    #     masked_part = img[:, y1:y2, x1:x2]\n",
    "    #     masked_img = img.clone()\n",
    "    #     masked_img[:, y1:y2, x1:x2] = 1\n",
    "\n",
    "    #     return masked_img, masked_part\n",
    "\n",
    "    # def apply_center_mask(self, img):\n",
    "    #     \"\"\"Mask center part of image\"\"\"\n",
    "    #     # Get upper-left pixel coordinate\n",
    "    #     i = (self.img_size - self.mask_size) // 2\n",
    "    #     masked_img = img.clone()\n",
    "    #     masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
    "\n",
    "    #     return masked_img, i\n",
    "    def apply_mask(self, original_img, mask_img):\n",
    "        # 确保掩码是单通道且大小匹配原始图像\n",
    "        if mask_img.shape[0] == 1:  # 如果是单通道\n",
    "            mask_img = mask_img.expand(3, -1, -1)  # 扩展为 3 通道\n",
    "        \n",
    "        # 提取被遮挡部分的图像\n",
    "        masked_part = original_img * mask_img\n",
    "        \n",
    "        # 创建被遮挡的图像，用1填充遮挡区域\n",
    "        masked_img = original_img.clone()\n",
    "        masked_img[mask_img == 1] = 1  # 用1替换遮挡区域\n",
    "        \n",
    "        if mask_img.shape[0] == 1:  # 如果是单通道\n",
    "            mask_img = mask_img.expand(3, -1, -1)  # 扩展到 3 通道\n",
    "\n",
    "        return masked_img, masked_part\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 处理原始图像\n",
    "        img = Image.open(self.original_files[index]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # 处理掩码图像\n",
    "        mask_img = Image.open(self.mask_files[index]).convert(\"L\")  # 转为灰度图\n",
    "        mask_img = self.transforms_mask(mask_img)\n",
    "        \n",
    "        # 确保掩码是二值化的\n",
    "        mask_img = (mask_img > 0.5).float()  # 掩码值为 0 或 1\n",
    "    \n",
    "        return img, mask_img\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beb43a2-1775-48c9-a7f5-231c406b22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 200\n",
    "        self.batch_size = 8\n",
    "        self.dataset_name = \"img_align_celeba\"\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 4\n",
    "        self.latent_dim = 100\n",
    "        self.img_size = 128\n",
    "        self.mask_size = 64\n",
    "        self.channels = 3\n",
    "        self.sample_interval = 500\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Opt(n_epochs={self.n_epochs}, batch_size={self.batch_size}, \"\n",
    "                f\"dataset_name='{self.dataset_name}', lr={self.lr}, b1={self.b1}, \"\n",
    "                f\"b2={self.b2}, n_cpu={self.n_cpu}, latent_dim={self.latent_dim}, \"\n",
    "                f\"img_size={self.img_size}, mask_size={self.mask_size}, \"\n",
    "                f\"channels={self.channels}, sample_interval={self.sample_interval})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e81cddf-de22-4ac0-a7f3-59d7e4a6fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "opt = Opt()\n",
    "\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch_h, patch_w = 16, 16  # 固定为16，以匹配 discriminator 的输出\n",
    "patch = (1, patch_h, patch_w)\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "pixelwise_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(channels=opt.channels)\n",
    "discriminator = Discriminator(channels=opt.channels)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    pixelwise_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Dataset loader\n",
    "transforms_ = [\n",
    "    transforms.Resize((opt.img_size, opt.img_size), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "def train_model(category):\n",
    "    dataloader = DataLoader(\n",
    "    ImageDataset(\"./dataset_train\", category, transforms_=transforms_),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        ImageDataset(\"./dataset_test\", category, transforms_=transforms_, mode=\"val\"),\n",
    "        batch_size=12,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "    )\n",
    "\n",
    "    for epoch in range(opt.n_epochs):\n",
    "        for i, (imgs, masks) in enumerate(dataloader):\n",
    "    \n",
    "            # Adversarial ground truths\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(imgs.shape[0], 1, patch_h, patch_w).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(Tensor(imgs.shape[0], 1, patch_h, patch_w).fill_(0.0), requires_grad=False)\n",
    "            \n",
    "            # Debugging: 打印出 valid 和 fake 的形状\n",
    "    \n",
    "             # Configure input\n",
    "            imgs = Variable(imgs.type(Tensor))\n",
    "            masks = Variable(masks.type(Tensor))\n",
    "        \n",
    "            # Create masked images (input for generator)\n",
    "            masked_imgs = imgs * (1 - masks) + masks  # 用掩码生成损坏的图像\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "    \n",
    "            optimizer_G.zero_grad()\n",
    "    \n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(masked_imgs, masks)\n",
    "    \n",
    "            \n",
    "    \n",
    "            # Adversarial and pixelwise loss\n",
    "            g_adv = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "            g_pixel = pixelwise_loss(gen_imgs * masks, imgs * masks)  # 只计算掩码区域的 pixel loss\n",
    "            g_loss = 0.001 * g_adv + 0.999 * g_pixel\n",
    "    \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "    \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "    \n",
    "            optimizer_D.zero_grad()\n",
    "    \n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = adversarial_loss(discriminator(imgs * masks), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach() * masks), fake)\n",
    "            d_loss = 0.5 * (real_loss + fake_loss)\n",
    "        \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            # print(\n",
    "            #     \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G adv: %f, pixel: %f]\"\n",
    "            #     % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_adv.item(), g_pixel.item())\n",
    "            # )\n",
    "        \n",
    "            # Generate sample at sample interval\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "    # 每个 epoch 结束后保存模型\n",
    "    model_path = f\"{category}_generator.pth\"\n",
    "    torch.save(generator.state_dict(), model_path)\n",
    "    print(f\"Saved generator model to {model_path}\")\n",
    "\n",
    "    model_path = f\"{category}_discriminator.pth\"\n",
    "    torch.save(discriminator.state_dict(), model_path)\n",
    "    print(f\"Saved discriminator model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9fde0-221f-463f-b8a5-9f6e8508bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1585716/2859763673.py:109: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  valid = Variable(Tensor(imgs.shape[0], 1, patch_h, patch_w).fill_(1.0), requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator model to face_generator.pth\n",
      "Saved discriminator model to face_discriminator.pth\n",
      "Saved generator model to scenario_generator.pth\n",
      "Saved discriminator model to scenario_discriminator.pth\n",
      "Saved generator model to street_scene_pairs_generator.pth\n",
      "Saved discriminator model to street_scene_pairs_discriminator.pth\n"
     ]
    }
   ],
   "source": [
    "# 依次训练每种类型的模型\n",
    "image_types = [\"face\", \"scenario\", \"street_scene_pairs\", \"texture\"]\n",
    "for image_type in image_types:\n",
    "    #train_model(image_type=image_type, num_epochs=10, batch_size=16, learning_rate=1e-5)\n",
    "    train_model(image_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "sd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
